# System Design & Architecture
## Disney Reviews RAG System

---

## 1. RAG Solution Overview

**The Challenge**: Enable customer experience teams to query 42,000+ Disneyland reviews using natural language and receive accurate, source-grounded answers.

**The Solution**: A Retrieval-Augmented Generation (RAG) system that combines vector similarity search (retrieval) with large language models (generation) to provide contextually accurate answers backed by actual customer reviews.

**One-Line RAG Flow Description**:  
*"Transform reviews into searchable vectors using tiktoken chunking and OpenAI embeddings, store in FAISS index, then retrieve relevant chunks for any query and feed them to GPT-4 to generate grounded answers."*

---

## 2. RAG Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG SYSTEM ARCHITECTURE                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PHASE 1: INDEXING (Offline)  â”‚      â”‚  PHASE 2: RETRIEVAL (Runtime)   â”‚
â”‚         Build Once              â”‚      â”‚      Query Many Times           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 â”‚      â”‚                                 â”‚
â”‚  ğŸ“„ Disney Reviews CSV          â”‚      â”‚  ğŸ’¬ User Query                  â”‚
â”‚  42,656 reviews                 â”‚      â”‚  "What do visitors say          â”‚
â”‚  3 locations (CA, Paris, HK)    â”‚      â”‚   about Hong Kong park?"        â”‚
â”‚                                 â”‚      â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                        â”‚
             â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ‚ï¸ TEXT CHUNKING               â”‚      â”‚  ğŸ”¢ QUERY EMBEDDING             â”‚
â”‚  Tool: tiktoken                 â”‚      â”‚  Tool: OpenAI API               â”‚
â”‚  â€¢ cl100k_base encoding         â”‚      â”‚  â€¢ text-embedding-3-small       â”‚
â”‚  â€¢ 500 tokens per chunk         â”‚      â”‚  â€¢ Same model as indexing       â”‚
â”‚  â€¢ 50 token overlap             â”‚      â”‚  â€¢ Output: 1536-dim vector      â”‚
â”‚  Output: ~45,610 chunks         â”‚      â”‚  Latency: ~80ms                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                        â”‚
             â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§® EMBEDDING GENERATION        â”‚      â”‚  ğŸ” VECTOR SEARCH               â”‚
â”‚  Tool: OpenAI API               â”‚      â”‚  Tool: FAISS                    â”‚
â”‚  â€¢ text-embedding-3-small       â”‚      â”‚  â€¢ IndexFlatL2 (L2 distance)    â”‚
â”‚  â€¢ Batch: 128-1800 chunks       â”‚      â”‚  â€¢ Search 45K+ vectors          â”‚
â”‚  â€¢ Output: [N Ã— 1536] matrix    â”‚      â”‚  â€¢ Return top-K nearest         â”‚
â”‚  â€¢ Cache: embeddings_N.npy      â”‚      â”‚  â€¢ K=5 (default)                â”‚
â”‚  Time: ~2-10 min (one-time)     â”‚      â”‚  Latency: <1ms                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                        â”‚
             â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¾ FAISS INDEX BUILD           â”‚      â”‚  ğŸ“‹ METADATA RETRIEVAL          â”‚
â”‚  Tool: FAISS (Facebook AI)      â”‚      â”‚  Tool: JSONL file               â”‚
â”‚  â€¢ IndexFlatL2 creation         â”‚      â”‚  â€¢ Load metadata for top-K      â”‚
â”‚  â€¢ Add all embeddings           â”‚      â”‚  â€¢ Branch, rating, location     â”‚
â”‚  â€¢ Save: faiss_N.index          â”‚      â”‚  â€¢ Review text chunks           â”‚
â”‚  â€¢ Size: ~265 MB (45K vectors)  â”‚      â”‚  Latency: <1ms                  â”‚
â”‚  Build time: <1 second          â”‚      â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                        â”‚
             â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ METADATA STORAGE            â”‚      â”‚  ğŸ“ PROMPT CONSTRUCTION         â”‚
â”‚  Format: JSONL                  â”‚      â”‚  Tool: Python string formatting â”‚
â”‚  â€¢ One JSON per line            â”‚      â”‚  â€¢ System instructions          â”‚
â”‚  â€¢ Aligned with FAISS index     â”‚      â”‚  â€¢ User query insertion         â”‚
â”‚  â€¢ Fields: review_id, branch,   â”‚      â”‚  â€¢ Retrieved context (top-K)    â”‚
â”‚    rating, location, chunk_text â”‚      â”‚  â€¢ Grounding rules              â”‚
â”‚  â€¢ Save: meta_N.jsonl           â”‚      â”‚  Latency: <1ms                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– LLM GENERATION                                                           â”‚
â”‚  Tool: OpenAI Chat Completions API                                          â”‚
â”‚  â€¢ Model: gpt-4o-mini                                                       â”‚
â”‚  â€¢ Temperature: 0.2 (low for consistency)                                   â”‚
â”‚  â€¢ Input: Prompt + Context (top-K chunks)                                   â”‚
â”‚  â€¢ Output: Grounded answer                                                  â”‚
â”‚  Latency: ~500ms                                                            â”‚
â”‚  Cost: ~$0.0001 per query                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  âœ… FINAL RESPONSE   â”‚
                      â”‚  â€¢ Answer text       â”‚
                      â”‚  â€¢ Source citations  â”‚
                      â”‚  â€¢ Metadata          â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. FastAPI Server Flow Diagram

This diagram shows how the FastAPI server orchestrates the RAG pipeline for incoming requests.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASTAPI SERVER REQUEST FLOW                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   CLIENT         â”‚
                           â”‚  (Browser/API)   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ HTTP POST /query
                                    â”‚ {"query": "...", "k": 5}
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            FASTAPI SERVER                                    â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. API ROUTE HANDLER (/query)                                     â”‚    â”‚
â”‚  â”‚     app/api/routes.py                                              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Receive HTTP request                                            â”‚    â”‚
â”‚  â”‚  â€¢ Parse JSON body                                                 â”‚    â”‚
â”‚  â”‚  â€¢ Extract: query, k, temperature, model                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. REQUEST VALIDATION                                             â”‚    â”‚
â”‚  â”‚     Pydantic Schema (QueryRequest)                                 â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Validate query: string, max 500 chars                           â”‚    â”‚
â”‚  â”‚  â€¢ Validate k: int, range 1-20, default=5                          â”‚    â”‚
â”‚  â”‚  â€¢ Validate temperature: float, range 0.0-2.0, default=0.2         â”‚    â”‚
â”‚  â”‚  â€¢ Validate model: str, default="gpt-4o-mini"                      â”‚    â”‚
â”‚  â”‚  âŒ If invalid â†’ Return 422 Error                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. METRICS TRACKING (Start)                                       â”‚    â”‚
â”‚  â”‚     app/utils/metrics.py                                           â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Start latency timer                                             â”‚    â”‚
â”‚  â”‚  â€¢ Log request metadata                                            â”‚    â”‚
â”‚  â”‚  â€¢ Record request count                                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. DEPENDENCY INJECTION                                           â”‚    â”‚
â”‚  â”‚     FastAPI Depends()                                              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Inject: query_handler (RAGQueryHandler)                         â”‚    â”‚
â”‚  â”‚  â€¢ Inject: settings (Config)                                       â”‚    â”‚
â”‚  â”‚  â€¢ Inject: metrics (MetricsCollector)                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG QUERY HANDLER                                        â”‚
â”‚                     app/services/rag_query.py                                â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  5. RETRIEVE CONTEXT (retrieve_context)                            â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  Step 5a: Embed Query                                              â”‚    â”‚
â”‚  â”‚    â€¢ Call OpenAI Embeddings API                                    â”‚    â”‚
â”‚  â”‚    â€¢ Model: text-embedding-3-small                                 â”‚    â”‚
â”‚  â”‚    â€¢ Output: [1 Ã— 1536] vector                                     â”‚    â”‚
â”‚  â”‚    â€¢ Time: ~80ms                                                   â”‚    â”‚
â”‚  â”‚                                                                     â”‚    â”‚
â”‚  â”‚  Step 5b: Search FAISS Index                                       â”‚    â”‚
â”‚  â”‚    â€¢ Load: self.index (FAISS IndexFlatL2)                          â”‚    â”‚
â”‚  â”‚    â€¢ Search: index.search(query_vector, k)                         â”‚    â”‚
â”‚  â”‚    â€¢ Output: indices=[10, 25, 42], distances=[0.23, 0.31, 0.42]    â”‚    â”‚
â”‚  â”‚    â€¢ Time: <1ms                                                    â”‚    â”‚
â”‚  â”‚                                                                     â”‚    â”‚
â”‚  â”‚  Step 5c: Load Metadata                                            â”‚    â”‚
â”‚  â”‚    â€¢ Load: self.metadata (from JSONL)                              â”‚    â”‚
â”‚  â”‚    â€¢ Extract: metadata[indices]                                    â”‚    â”‚
â”‚  â”‚    â€¢ Output: List of review chunks with branch, rating, etc.       â”‚    â”‚
â”‚  â”‚    â€¢ Time: <1ms                                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  6. BUILD PROMPT (build_prompt)                                    â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Template: System instruction                                    â”‚    â”‚
â”‚  â”‚  â€¢ Insert: User query                                              â”‚    â”‚
â”‚  â”‚  â€¢ Insert: Retrieved context (top-K chunks)                        â”‚    â”‚
â”‚  â”‚  â€¢ Add: Grounding instructions                                     â”‚    â”‚
â”‚  â”‚  â€¢ Output: Complete prompt string (~800-1200 tokens)               â”‚    â”‚
â”‚  â”‚  â€¢ Time: <1ms                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  7. GENERATE ANSWER (generate_answer)                              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Call OpenAI Chat Completions API                                â”‚    â”‚
â”‚  â”‚  â€¢ Model: gpt-4o-mini (or user-specified)                          â”‚    â”‚
â”‚  â”‚  â€¢ Temperature: 0.2 (or user-specified)                            â”‚    â”‚
â”‚  â”‚  â€¢ Input: Prompt with context                                      â”‚    â”‚
â”‚  â”‚  â€¢ Output: Generated answer text                                   â”‚    â”‚
â”‚  â”‚  â€¢ Time: ~500ms                                                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  8. PACKAGE RESPONSE (query)                                       â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Combine: answer + retrieval_results                             â”‚    â”‚
â”‚  â”‚  â€¢ Add metadata: k, model, temperature                             â”‚    â”‚
â”‚  â”‚  â€¢ Format: QueryResponse (Pydantic model)                          â”‚    â”‚
â”‚  â”‚  â€¢ Time: <1ms                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BACK TO FASTAPI SERVER                                   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  9. METRICS TRACKING (End)                                         â”‚    â”‚
â”‚  â”‚     app/utils/metrics.py                                           â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Stop latency timer                                              â”‚    â”‚
â”‚  â”‚  â€¢ Calculate total latency: ~600ms                                 â”‚    â”‚
â”‚  â”‚  â€¢ Record retrieval distance avg: 0.32                             â”‚    â”‚
â”‚  â”‚  â€¢ Update metrics: success count, latency histogram                â”‚    â”‚
â”‚  â”‚  â€¢ Log completion                                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â”‚                           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  10. RESPONSE SERIALIZATION                                        â”‚    â”‚
â”‚  â”‚      Pydantic â†’ JSON                                               â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  â€¢ Convert QueryResponse to JSON                                   â”‚    â”‚
â”‚  â”‚  â€¢ Add HTTP headers (Content-Type: application/json)               â”‚    â”‚
â”‚  â”‚  â€¢ Add CORS headers (if enabled)                                   â”‚    â”‚
â”‚  â”‚  â€¢ Time: <1ms                                                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP 200 OK
                            â”‚ JSON Response
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   CLIENT         â”‚
                     â”‚  (Browser/API)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### Key FastAPI Features Used

| Feature | Purpose | Implementation |
|---------|---------|----------------|
| **Pydantic Models** | Request/Response validation | `QueryRequest`, `QueryResponse` schemas |
| **Dependency Injection** | Share components across routes | `Depends(get_query_handler)` |
| **Lifespan Events** | Load FAISS index on startup | `@asynccontextmanager` for app lifespan |
| **CORS Middleware** | Enable cross-origin requests | `app.add_middleware(CORSMiddleware)` |
| **Automatic Docs** | Interactive API documentation | Swagger UI at `/docs` |
| **Async/Await** | Non-blocking I/O for OpenAI calls | `async def query_endpoint()` |

### Error Handling Flow

```
Request â†’ Validation Error (422)
       â†’ OpenAI API Error (503)
       â†’ FAISS Index Not Loaded (503)
       â†’ Internal Server Error (500)
       â†’ Success (200)
```

## 4. RAG Components Deep Dive

### 4.1 Text Chunking (tiktoken)

**Purpose**: Split long reviews into manageable, semantically coherent pieces.

**Tool**: `tiktoken` - OpenAI's fast tokenizer library

### 4.2 Embeddings (OpenAI API)

**Purpose**: Convert text chunks into high-dimensional vectors that capture semantic meaning.

**Tool**: OpenAI Embeddings API
- **Model**: `text-embedding-3-small`
- **Dimensions**: 1,536 per vector
- **Quality**: State-of-the-art semantic understanding
- **Cost**: $0.02 per 1M tokens (~$0.42 for full dataset)

**Why text-embedding-3-small**:
- âœ… Best quality-to-cost ratio
- âœ… High semantic accuracy
- âœ… Consistent with OpenAI's LLM ecosystem
- âœ… 1,536 dimensions (optimal for FAISS)


**Optimization**:
- Caching: Save to `.npy` files to avoid regenerating
- Parallel processing: Use async/threading for 4-8x speedup
- Rate limiting: Respect OpenAI's 1M tokens/minute limit

**Output**: NumPy array of shape `[45,610, 1,536]`

### 4.3 Vector Index (FAISS)

**Purpose**: Enable fast similarity search over thousands of embedding vectors.

**Tool**: FAISS (Facebook AI Similarity Search)
- **Index Type**: `IndexFlatL2` (exact L2 distance search)
- **Distance Metric**: Euclidean (L2) distance
- **Why FAISS**: Ultra-fast (<1ms), no external dependencies, handles millions of vectors

**IndexFlatL2 Characteristics**:
- âœ… **Exact search**: 100% recall accuracy
- âœ… **Fast**: <1ms for 45K vectors, <10ms for 1M vectors
- âœ… **Simple**: No training or tuning required
- âŒ **Memory**: ~265 MB for 45K vectors (6 bytes per dimension)

**Alternative Index Types** (for future scaling):
- `IndexIVFFlat`: Approximate search, 10-100x faster for millions of vectors
- `IndexHNSWFlat`: Graph-based, excellent for high-dimensional data

**Search Performance**:
```python
# Query: Find top-K most similar chunks


### 4.5 LLM Generation (OpenAI Chat Completions)

**Purpose**: Generate natural language answers grounded in retrieved context.

**Tool**: OpenAI Chat Completions API
- **Model**: `gpt-4o-mini`
- **Why gpt-4o-mini**: Best cost/performance balance ($0.15/1M input tokens vs $5/1M for GPT-4)
- **Temperature**: 0.2 (low for factual consistency)
- **Max tokens**: 500-1000 (configurable)


## 5. Complete RAG Flow (Step-by-Step)

### Phase 1: Indexing (One-time setup, ~10 minutes)

```
Step 1: Load Data
â”œâ”€ Read: data/DisneylandReviews.csv
â”œâ”€ Sample: 10,000 reviews (configurable)
â””â”€ Output: DataFrame with 10,000 rows

Step 2: Chunk Text
â”œâ”€ Tool: tiktoken (cl100k_base)
â”œâ”€ Process: 10,000 reviews â†’ 10,700 chunks
â”œâ”€ Params: 500 tokens/chunk, 50 token overlap
â””â”€ Output: List of 10,700 text chunks

Step 3: Generate Embeddings
â”œâ”€ Tool: OpenAI text-embedding-3-small
â”œâ”€ Batch: 128-1800 chunks per API call
â”œâ”€ Process: 10,700 chunks â†’ 10,700 vectors
â”œâ”€ Output: [10,700 Ã— 1,536] NumPy array
â”œâ”€ Time: ~2-10 minutes
â””â”€ Cost: ~$0.05

Step 4: Build FAISS Index
â”œâ”€ Tool: FAISS IndexFlatL2
â”œâ”€ Add: 10,700 vectors to index
â”œâ”€ Save: rag_index/faiss_10000.index
â”œâ”€ Time: <1 second
â””â”€ Size: ~62 MB

Step 5: Save Metadata
â”œâ”€ Format: JSONL
â”œâ”€ Save: rag_index/meta_10000.jsonl
â”œâ”€ Fields: review_id, branch, rating, location, year_month, chunk
â””â”€ Size: ~15 MB
```

### Phase 2: Retrieval (Every query, ~600ms)

```
Step 1: Receive Query
â”œâ”€ Input: "What do visitors like about Hong Kong park?"
â”œâ”€ Validation: Check query length, sanitize
â””â”€ Time: <1ms

Step 2: Embed Query
â”œâ”€ Tool: OpenAI text-embedding-3-small
â”œâ”€ Process: Query text â†’ 1536-dim vector
â”œâ”€ Output: [1 Ã— 1,536] vector
â”œâ”€ Time: ~80ms
â””â”€ Cost: ~$0.000001

Step 3: Search FAISS Index
â”œâ”€ Tool: FAISS IndexFlatL2
â”œâ”€ Input: Query vector + k=5
â”œâ”€ Process: Compare with 10,700 vectors
â”œâ”€ Output: 5 nearest neighbor indices + distances
â”œâ”€ Example: indices=[42, 156, 891, 1203, 3456]
â”‚           distances=[0.23, 0.31, 0.42, 0.48, 0.52]
â””â”€ Time: <1ms

Step 4: Retrieve Metadata
â”œâ”€ Tool: JSONL file reading
â”œâ”€ Load: Metadata for indices [42, 156, 891, 1203, 3456]
â”œâ”€ Output: 5 review chunks with branch, rating, location
â””â”€ Time: <1ms

Step 5: Build Prompt
â”œâ”€ Template: System instruction + Query + Context
â”œâ”€ Insert: User query
â”œâ”€ Insert: Top-5 review chunks with metadata
â”œâ”€ Output: Complete prompt (~800-1200 tokens)
â””â”€ Time: <1ms

Step 6: Generate Answer
â”œâ”€ Tool: OpenAI gpt-4o-mini
â”œâ”€ Input: Prompt with context
â”œâ”€ Params: temperature=0.2, max_tokens=500
â”œâ”€ Process: LLM generates grounded answer
â”œâ”€ Output: Answer text (~100-300 words)
â”œâ”€ Time: ~500ms
â””â”€ Cost: ~$0.0001

Step 7: Return Response
â”œâ”€ Package: Answer + sources + metadata
â”œâ”€ Format: JSON response
â”œâ”€ Fields: query, answer, retrieval_results, model, k, temperature
â””â”€ Time: <1ms

Total Query Latency: ~600ms (p50), ~1200ms (p95)
Total Query Cost: ~$0.0001
``

## 6. RAG Performance Metrics

### Indexing Metrics (One-time)

| Metric | Value | Notes |
|--------|-------|-------|
| **Input reviews** | 42,656 | Full dataset |
| **Output chunks** | 45,610 | After tiktoken chunking |
| **Embedding dimensions** | 1,536 | per chunk |
| **Index size** | 265 MB | FAISS + metadata |
| **Build time** | 8-12 min | With embedding generation |
| **Cost** | $0.42 | OpenAI embeddings |

### Query Metrics (Per request)

| Component | Latency | Cost | Tool |
|-----------|---------|------|------|
| **Query embedding** | ~80ms | $0.000001 | OpenAI API |
| **FAISS search** | <1ms | $0 | Local |
| **Metadata retrieval** | <1ms | $0 | Local |
| **LLM generation** | ~500ms | $0.0001 | OpenAI API |
| **Total (p50)** | **~600ms** | **$0.0001** | - |
| **Total (p95)** | **~1200ms** | **$0.0001** | - |

### Quality Metrics

| Metric | Good | Fair | Poor |
|--------|------|------|------|
| **Retrieval distance** | <0.5 | 0.5-0.7 | >0.7 |
| **Answer relevance** | High | Medium | Low |
| **Source citations** | âœ… Always | âœ… Always | âœ… Always |

---

## 7. Technology Stack Summary

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Tokenization** | tiktoken | latest | Chunk text into tokens |
| **Embeddings** | OpenAI API | text-embedding-3-small | Convert text to vectors |
| **Vector Search** | FAISS | 1.7.4+ | Fast similarity search |
| **Metadata** | JSONL | - | Store review metadata |
| **LLM** | OpenAI API | gpt-4o-mini | Generate answers |
| **API Framework** | FastAPI | 0.119+ | Serve HTTP endpoints |
| **UI** | Gradio | 4.0+ | Interactive web interface |
| **Language** | Python | 3.10+ | Implementation language |


## 8. RAG Trade-offs & Design Decisions

### Why Exact Search (IndexFlatL2) vs Approximate?

**Decision**: Use `IndexFlatL2` (exact search)

**Rationale**:
- âœ… **100% recall**: No missed relevant results
- âœ… **Simple**: No training or parameter tuning
- âœ… **Fast enough**: <1ms for 45K vectors
- âœ… **Quality first**: Accuracy more important than speed at this scale

**Trade-offs**:
- âŒ Slower for >1M vectors (but still <10ms)
- âœ… Can switch to `IndexIVFFlat` later if needed

---

### Why 500-token chunks with 50-token overlap?

**Decision**: 500 tokens per chunk, 50-token overlap

**Rationale**:
- âœ… **Semantic coherence**: 500 tokens = ~1-2 paragraphs (enough context)
- âœ… **Embedding quality**: Within sweet spot for text-embedding-3-small
- âœ… **Overlap**: Prevents losing information at boundaries
- âœ… **Retrieval precision**: Smaller chunks = more precise matching

**Trade-offs**:
- Larger chunks (1000 tokens): More context but less precise
- Smaller chunks (200 tokens): More precise but fragmented context
- More overlap (100 tokens): Better continuity but more redundancy

---

### Why gpt-4o-mini vs GPT-4?

**Decision**: Use `gpt-4o-mini` as default LLM

**Rationale**:
- âœ… **Cost**: $0.15/1M input tokens vs $5/1M for GPT-4 (33x cheaper)
- âœ… **Speed**: ~500ms vs ~1-2s for GPT-4
- âœ… **Quality**: Sufficient for factual QA with provided context
- âœ… **Flexibility**: Users can override with GPT-4 if needed

**Trade-offs**:
- GPT-4 has slightly better reasoning, but the difference is minimal for RAG tasks where context is provided

---

### Why FAISS vs Vector Databases (Pinecone, Weaviate)?

**Decision**: Use FAISS (local, in-memory)

**Rationale**:
- âœ… **No external dependencies**: Works offline, no API costs
- âœ… **Fast**: <1ms search latency (in-memory)
- âœ… **Simple deployment**: Just load index file
- âœ… **Cost**: Free (vs $70-100/month for managed vector DBs)
- âœ… **Proven**: Battle-tested by Facebook AI Research

**Trade-offs**:
- Vector DBs offer features like filtering, updates, distributed search
- For 45K-1M vectors, FAISS is optimal
- Can migrate to vector DB if scale requires it (>10M vectors)

---

**Document Version**: 1.0  
**Last Updated**: October 29, 2025  
**Focus**: RAG Solution Architecture

